<h1 id="overall-idea">Overall Idea</h1>
<p>I am looking at load balancing, possibly in edge clusters, as soft real time system. This system might have deadlines for different processess or for processes generated by certain sources. But may not have any deadline requirements for many others.</p>
<ul>
<li><p>Look at the processing of various processes of the system and form different clusters to form a loose definition of different states of the system.</p>
<ul>
<li>Advantage is that this can allow us define a finite number of states for any system.</li>
<li>A first attempt can select number of clusters based on number of different requests that can be made to the system. However, higher dimensional cluster like a combination of request types can also be considered.</li>
</ul></li>
<li><p>Once a determination of states are made, use reinforcement learning to find an optimal scheduling solution that looks to reduce tardiness given a generic optimization goal.</p>
<ul>
<li>Optimization goal can be provided in terms of a general service time.</li>
<li>tardiness := <span class="math inline"><em>m</em><em>a</em><em>x</em>[0, (<em>t</em> − <em>d</em>)]</span>
<ol type="1">
<li>t -&gt; total time spent in the system by a process</li>
<li>d -&gt; deadline for the process. If the process does not have a specific deadline then d defaults to the optimization goal.</li>
</ol></li>
</ul></li>
</ul>
<h2 id="plan">Plan</h2>
<h3 id="project-equations">Project equations</h3>
<p><br /><span class="math display">$$Goal := min \sum_{i=1}^{N(T)} max[0, \frac{(t_{ij} - d_{ij})}{d_{ij}}]$$</span><br /> So,</p>
<p><br /><span class="math display">$$G_t = max \sum_{i=1}^{N(T)} max[0, -\frac{(t_{ij} - d_{ij})}{d_{ij}}]$$</span><br /> where,</p>
<p><br /><span class="math display"><em>d</em><sub><em>i</em><em>j</em></sub> = <em>μ</em><sub><em>t</em><sub><em>i</em><em>j</em></sub></sub> + <em>c</em><sub><em>j</em></sub><em>σ</em><sub><em>t</em><sub><em>i</em><em>j</em></sub></sub></span><br /> <br /><span class="math display">$$ c_j = 1 + \frac {1} {p_j}$$</span><br /> <span class="math inline"><em>T</em></span> is the time interval at which the reward is measured and <span class="math inline"><em>N</em>(<em>T</em>)</span> is the number of jobs that go through that system in that time.</p>
<p>where <span class="math inline"><em>p</em><sub><em>i</em><em>j</em></sub></span> := priority of task type j and <span class="math inline"><em>c</em><sub><em>j</em></sub> ∈ (1, 2)</span></p>
<h3 id="storing-states">Storing states</h3>
<p>There can be a predefined number of distinct states, K. Each state can be stored as a mean vector <span class="math inline"><em>μ̄</em></span> and a covariance matrix <span class="math inline"><em>Σ</em></span>. The states can be recalibrated if the number of observed data points that lie outside all states; <span class="math inline">$\bar{s} \notin \{\bar{\mu_k} \pm 3\Sigma_k\} \forall k\in K$</span>; cross a predefined threshold, <span class="math inline"><em>N</em>(<em>k</em>)</span>. Recalibration will include another observation stage.</p>
<h3 id="load-balancing">Load Balancing</h3>
<p>The Weighted Round Robin (WRR) forms the basis of the load balancing algorithm. Different types of tasks have different priorities. For each task type, the backend servers can have different weights. The idea is to readjust the weights of the servers, for each task type, so that the overall resources are distributed between the jobs in the system so as to maximize <span class="math inline"><em>G</em><sub><em>t</em></sub></span>.</p>
<h3 id="training-the-rl-model">Training the RL model</h3>
<h4 id="todo">TODO</h4>

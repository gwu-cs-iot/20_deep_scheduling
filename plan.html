<h1 id="reducing-accumulative-relative-tardiness-art-for-soft-realtime-load-balancing">Reducing Accumulative Relative Tardiness (ART) for soft realtime load balancing</h1>
<h2 id="abstract">Abstract</h2>
<p>Many systems today have incorporated in them the idea of task priorities and have a growing requirement of soft real-time guarantees while making scheduling (load balancing) decisions. In this project, I am looking at the idea of using reinforcement learning to tune weights of the weighted round robin (WRR) load balancer to distribute resources between tasks to minimize the overall tardiness in a system.</p>
<h2 id="introduction">Introduction</h2>
<p>I am looking at load balancing, possibly in edge clusters, as a soft real time system. This system might have different deadlines for different processes or for processes generated by certain sources. But may not have any deadline requirements for many others.</p>
<p>There are two distinct parts of the system:</p>
<ul>
<li><p>a <strong><em>load balancer</em></strong></p>
<ul>
<li>This is responsible for actually balancing load for the different upstream servers. Servers that can satisfy a particular request type will be grouped together. We will call these <strong><em>service groups</em></strong>. Each server in a group will be given a weight.</li>
</ul></li>
<li><p>a <strong><em>scheduler</em></strong></p>
<ul>
<li>The scheduler looks at the aggregate state of the load balancer. There will be an initial observation period during which will end when the scheduler has enough data points. This will be manually decided. Once the scheduler has enough data points, the scheduler will club together similar data points into clusters. Each cluster will now behave as a state for the decision making stage of the scheduler. The overall idea is to get the scheduler to tweak the weights in different service groups to move the load balancer from one state to another more desirable state.</li>
</ul></li>
<li><p>Observe and collect statistics from the load balancer to create different clusters to broadly define different “states” of the system.</p>
<ul>
<li>This allows us to define a finite number of states.</li>
<li>The number of clusters have to be pre-defined. The dimensionality of the data used is a good guide in that direction.</li>
</ul></li>
<li><p>Once states are determined, use reinforcement learning to find an optimal scheduling solution that looks to reduce tardiness given optimization goals.</p>
<ul>
<li>Optimization goals will be provided in terms of service times.</li>
<li><strong><em>tardiness</em></strong> := <span class="math inline"><em>m</em><em>a</em><em>x</em>[0, (<em>t</em> − <em>d</em>)]</span>
<ol type="1">
<li><span class="math inline"><em>t</em> = &gt;</span> total time spent in the system by a job</li>
<li><span class="math inline"><em>d</em> = &gt;</span> deadline for the process.</li>
</ol></li>
</ul></li>
</ul>
<h2 id="project-goal">Project Goal</h2>
<p><br /><span class="math display">$$Goal := min \sum_{i=1}^{N(T)} max[0, \frac{(t_{ij} - d_j)}{d_j}]$$</span><br /> (1a)</p>
<p>So,</p>
<p><br /><span class="math display">$$G_t = max \sum_{i=1}^{N(T)} min[0, \frac{(d_j - t_{ij})}{d_j}]$$</span><br /> (1b)</p>
<p>where,</p>
<p><br /><span class="math display"><em>d</em><sub><em>j</em></sub> = <em>μ</em><sub><em>t</em><sub><em>j</em></sub></sub> + <em>c</em><sub><em>j</em></sub><em>σ</em><sub><em>t</em><sub><em>j</em></sub></sub></span><br /> (2a) <br /><span class="math display">$$\mu_{t_j} = \frac{1}{n_j}\sum_it_{ij}1(j)$$</span><br /> (2b) <br /><span class="math display">$$\sigma_{t_j} = [\frac{1}{n_j}\sum_it_{ij}^21(j)] - \mu_{t_j}$$</span><br /> (2c) <br /><span class="math display"><em>t</em><sub><em>j</em></sub> = ∑<sub><em>i</em></sub><em>t</em><sub><em>j</em></sub>1(<em>j</em>)</span><br /> (2d)</p>
<p><br /><span class="math display">$$
1(j) =
\begin{cases}
    1,&amp; \text{if task is of type j}\\
    0,              &amp; \text{otherwise}
\end{cases}
$$</span><br /></p>
<p><br /><span class="math display">$$ c_j = 1 + \frac {1} {p_j}$$</span><br /> (2f)</p>
<p><span class="math inline"><em>T</em></span> is the <strong><em>time interval at which the reward is measured</em></strong> and <span class="math inline"><em>N</em>(<em>T</em>)</span> is the <strong><em>number of jobs that go through that system in that time</em></strong>.</p>
<p>where:</p>
<p><br /><span class="math display"><em>p</em><sub><em>j</em></sub> := priority of task type j</span><br /> <br /><span class="math display"><em>c</em><sub><em>j</em></sub> ∈ (1, 2)</span><br /> <br /><span class="math display"><em>μ</em><sub><em>t</em><sub><em>j</em></sub></sub>is the mean of all tasks of type j completed when the scheduler was observing the system</span><br /> <br /><span class="math display"><em>σ</em><sub><em>t</em><sub><em>j</em></sub></sub>is the standard deviation of all tasks of type j completed when the scheduler was observing the system</span><br /></p>
<p><span class="math inline">$\frac{(t_{ij} - d_{ij})}{d_{ij}}$</span> is the <strong><em>Relative Tardiness</em></strong> of the job.</p>
<p>We are interested in minimizing the <strong><em>Accumulative Relative Tardiness (ART)</em></strong> - <span class="math inline">$\sum_{i=1}^{N(T)} max[0, \frac{(t_{ij} - d_{ij})}{d_{ij}}]$</span> of the overall system.</p>
<h2 id="states-and-storing-them">States and storing them</h2>
<p>The system will be under an initial period of observation while it’s under scripted load. These observations are to be used to create a broad definition of “states” which will consist of a bunch of similar actual states of the system. We will add the actual parameters later.</p>
<p>There can be a predefined number of distinct states K, possibly depending on the dimensionality of the data being considered. Each state can be stored as a mean vector <span class="math inline"><strong>μ</strong></span> and a covariance matrix <span class="math inline"><em>Σ</em></span>. The states can be recalibrated if the number of observed data points that lie outside all states; <span class="math inline"><strong>s</strong> ∉ {<strong>μ</strong><sub><em>k</em></sub> ± 3<em>Σ</em><sub><em>k</em></sub>}∀<em>k</em> ∈ [1, <em>K</em>]</span>; cross a predefined threshold, <span class="math inline"><em>N</em><sub><em>t</em><em>h</em><em>r</em><em>e</em><em>s</em><em>h</em><em>o</em><em>l</em><em>d</em></sub></span>. Recalibration will include another observation stage.</p>
<h3 id="state-parameters">State parameters</h3>
<p><strong><em>For each request type and for each server.</em></strong> These counters are specific to NGINX load balancers.</p>
<ul>
<li>requestMsec
<ul>
<li>The average of request processing times in milliseconds. requestMsecs</li>
<li>times
<ul>
<li>The times in milliseconds at request processing times.</li>
</ul></li>
<li>msecs
<ul>
<li>The request processing times in milliseconds.</li>
</ul></li>
</ul></li>
<li>requestBuckets
<ul>
<li>msecs</li>
<li>The bucket values of histogram set by vhost_traffic_status_histogram_buckets directive.</li>
<li>counters
<ul>
<li>The cumulative values for the reason that each bucket value is greater than or equal to the request processing time including upstream.</li>
</ul></li>
</ul></li>
<li>connections
<ul>
<li>active
<ul>
<li>The current number of active client connections.</li>
</ul></li>
<li>reading
<ul>
<li>The total number of reading client connections.</li>
</ul></li>
<li>writing
<ul>
<li>The total number of writing client connections.</li>
</ul></li>
<li>waiting
<ul>
<li>The total number of waiting client connections.</li>
</ul></li>
<li>accepted
<ul>
<li>The total number of accepted client connections.</li>
</ul></li>
<li>handled
<ul>
<li>The total number of handled client connections.</li>
</ul></li>
<li>requests
<ul>
<li>The total number of requested client connections.</li>
</ul></li>
</ul></li>
<li>requestCounter
<ul>
<li>The total number of client connections forwarded to this server.</li>
</ul></li>
<li>responses
<ul>
<li>1xx, 2xx, 3xx, 4xx, 5xx
<ul>
<li>The number of responses with status codes 1xx, 2xx, 3xx, 4xx, and 5xx.</li>
</ul></li>
</ul></li>
<li>weight
<ul>
<li>Current weight setting of the server.</li>
</ul></li>
<li>maxFails
<ul>
<li>Current max_fails setting of the server.</li>
</ul></li>
</ul>
<h2 id="load-balancing">Load Balancing</h2>
<p>The Weighted Round Robin (WRR) forms the basis of the load balancing algorithm. Different types of tasks have different priorities. For each task type, the backend servers can have different weights. The idea is to readjust the weights of the servers, for each task type, so that the overall resources are distributed between the jobs in the system so as to maximize <span class="math inline"><em>G</em><sub><em>t</em></sub></span>.</p>
<h2 id="role-of-the-wrr-weights-as-our-control">Role of the WRR weights as our control</h2>
<p>Each backend server is allocated a weight for each request type. For example, server <span class="math inline"><em>i</em></span> will have the weight <span class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span> for task type <span class="math inline"><em>j</em></span>.</p>
<p>Total share of a particular server for a particular task type is <span class="math inline">$\frac{w_{ij}}{\sum_jw_{ij}}$</span>.</p>
<p>Total share of resources allocated to any task <span class="math inline"><em>j</em></span> is:</p>
<p><br /><span class="math display">$$R_j = \sum_i(\frac{w_{ij}}{\sum_jw_{ij}})$$</span><br /> (3) Since, <span class="math inline">∑<sub><em>j</em></sub><em>R</em><sub><em>j</em></sub> = <em>S</em></span>, where <span class="math inline"><em>S</em></span> is the total number of servers, adjusting the weights in a WRR provides a way to control the amount of total resources provided to a task type.</p>
<p>(The assumption here is that the servers are homogeneous.)</p>
<h2 id="role-of-the-reinforcement-learning-model">Role of the Reinforcement Learning Model</h2>
<p>Now that we have our states, a goal and our control, the role of the reinforcement learning model would be to learn how to tweak the controls in order to move towards the goal along the best possible path.</p>
<h2 id="training-the-rl-model">Training the RL model</h2>
<h3 id="major-issue">Major issue</h3>
<p>The desirable states may be unobservable unless states are observed and classified during training.</p>
<h3 id="todo">TODO</h3>
<p>How to train the model? Especially with simulation.</p>
